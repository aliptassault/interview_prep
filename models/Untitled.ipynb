{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d9780-9746-4edc-a286-4ad971281f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema.runnable import RunnableWithMessageHistory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.agents.agent_toolkits import Tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load and split documents\n",
    "doc_loader = DirectoryLoader(\"./enterprise_docs\", loader_cls=TextLoader)\n",
    "documents = doc_loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n",
    "# Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector DB\n",
    "faiss_db = FAISS.from_documents(split_docs, embedding_model)\n",
    "retriever = faiss_db.as_retriever()\n",
    "\n",
    "# Chat Models\n",
    "llm_models = {\n",
    "    \"gpt-4\": ChatOpenAI(model_name=\"gpt-4\", temperature=0.3),\n",
    "    \"claude-3-sonnet\": HuggingFaceEndpoint(endpoint_url=\"https://api.anthropic.com/claude-3-sonnet\", temperature=0.3),\n",
    "    \"gemma2-9b-it\": HuggingFaceEndpoint(endpoint_url=\"https://api.huggingface.co/gemma2-9b-it\", temperature=0.3),\n",
    "    \"mistral\": HuggingFaceEndpoint(endpoint_url=\"https://api.huggingface.co/mistral\", temperature=0.3),\n",
    "    \"deepseek\": HuggingFaceEndpoint(endpoint_url=\"https://api.huggingface.co/deepseek\", temperature=0.3),\n",
    "}\n",
    "\n",
    "# Select active model\n",
    "def get_llm(model_name=\"gpt-4\"):\n",
    "    return llm_models.get(model_name, llm_models[\"gpt-4\"])\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompts\n",
    "rag_prompt = PromptTemplate.from_template(\n",
    "    \"Answer the question using the context below:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "fallback_prompt = PromptTemplate.from_template(\n",
    "    \"Answer the question using your own knowledge:\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Logging setup\n",
    "LOG_FILE = \"unanswered_queries.json\"\n",
    "unanswered_log = []\n",
    "\n",
    "def log_unanswered_query(query):\n",
    "    if not os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"w\") as f:\n",
    "            json.dump([], f)\n",
    "    with open(LOG_FILE, \"r+\") as f:\n",
    "        data = json.load(f)\n",
    "        data.append({\"query\": query})\n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "# LangGraph nodes\n",
    "def retrieve_context(state):\n",
    "    query = state[\"question\"]\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    if not docs:\n",
    "        unanswered_log.append(query)\n",
    "        log_unanswered_query(query)\n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"docs\": docs,\n",
    "        \"chat_history\": state.get(\"chat_history\", []),\n",
    "        \"model_name\": state.get(\"model_name\", \"gpt-4\")\n",
    "    }\n",
    "\n",
    "def route(state):\n",
    "    return \"rag_chain\" if state[\"docs\"] else \"fallback_chain\"\n",
    "\n",
    "def run_rag(state):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in state[\"docs\"])\n",
    "    model = get_llm(state[\"model_name\"])\n",
    "    chain = LLMChain(llm=model, prompt=rag_prompt)\n",
    "    answer = chain.run({\"context\": context, \"question\": state[\"question\"]})\n",
    "    return {\"answer\": answer, \"chat_history\": state[\"chat_history\"] + [state[\"question\"], answer]}\n",
    "\n",
    "def run_fallback(state):\n",
    "    model = get_llm(state[\"model_name\"])\n",
    "    chain = LLMChain(llm=model, prompt=fallback_prompt)\n",
    "    answer = chain.run({\"question\": state[\"question\"]})\n",
    "    return {\"answer\": answer, \"chat_history\": state[\"chat_history\"] + [state[\"question\"], answer]}\n",
    "\n",
    "# Define and compile graph\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"retrieve\", retrieve_context)\n",
    "graph.add_conditional_edges(\"retrieve\", route)\n",
    "graph.add_node(\"rag_chain\", run_rag)\n",
    "graph.add_node(\"fallback_chain\", run_fallback)\n",
    "graph.add_edge(\"rag_chain\", END)\n",
    "graph.add_edge(\"fallback_chain\", END)\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "compiled_graph = graph.compile()\n",
    "\n",
    "# Session-aware chatbot interface\n",
    "chatbot_graph = RunnableWithMessageHistory(\n",
    "    compiled_graph,\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Sample run\n",
    "if __name__ == \"__main__\":\n",
    "    session_id = \"user_123\"\n",
    "    user_question = \"What are the compliance rules for remote employees?\"\n",
    "    result = chatbot_graph.invoke(\n",
    "        {\"question\": user_question, \"model_name\": \"claude-3-sonnet\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "\n",
    "    if unanswered_log:\n",
    "        print(\"\\nðŸ›‘ Unanswered Queries Logged:\")\n",
    "        for q in unanswered_log:\n",
    "            print(\" -\", q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
