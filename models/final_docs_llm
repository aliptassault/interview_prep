# from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain_community.llms.llamacpp import LlamaCpp
from langchain.memory import ConversationBufferMemory
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, AgentType
# from langchain.agents.agent_toolkits import Tool
from langgraph.graph import StateGraph, END
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import json 
import os
from langchain.tools import Tool
from langchain.chains.llm import LLMChain
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import shutil
from llama_cpp import Llama
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from typing import TypedDict, List, Optional, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing import TypedDict, List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain_groq import ChatGroq



os.environ["OMP_NUM_THREADS"] = "16"
os.environ["MKL_NUM_THREADS"] = "16"
num_workers = multiprocessing.cpu_count()

# def create_llm():
#     # Initialize the model once
#     return LlamaCpp(
#         model_path="./models/gemma-7b.gguf",
#         n_gpu_layers=40,
#         n_batch=1024,
#         n_ctx=4096,
#         f16_kv=True,
#         verbose=False,
#         n_threads=16,
#         gpu_layers_max_thread_use=16,
#         temperature=0.3,
#         mlock=True  # Keep model in memory
#     )
groq_api_key = "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"
def create_llm():
    return ChatGroq(groq_api_key=groq_api_key,model_name="Llama3-8b-8192")
# Initialize model once at startup
try:
    GLOBAL_LLM = create_llm()
    print("Global LLM model initialized successfully")
except Exception as e:
    print(f"Error initializing global LLM: {e}")
    raise

# Load and split documents
doc_loader = PyPDFDirectoryLoader("./enterprise_docs", glob="**/*.pdf")
# Print documents present in the directory
print("Documents in directory:", os.listdir("enterprise_docs"), doc_loader)
documents = doc_loader.load()

# Check if documents are loaded successfully
if not documents:
    raise ValueError("No documents found in the specified directory. Please check the directory path and ensure it contains valid files.")

# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
# split_docs = splitter.split_documents(documents)
# Parallel document splitting
def split_doc_batch(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    return splitter.split_documents(docs)

def parallel_split_documents(documents, batch_size=100):
    batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        split_docs = list(executor.map(split_doc_batch, batches))
    return [doc for batch in split_docs for doc in batch]

# Use parallel splitting
split_docs = parallel_split_documents(documents)
print("Number of split documents:", len(split_docs))
# Embeddings
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
print("Embedding model loaded successfully.")
# Vector DB
# shutil.rmtree("./chroma_langchain_db", ignore_errors=True)  # Deletes existing persisted data
vector_store = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding_model,
    collection_name="example_collection",
    persist_directory="./chroma_langchain_db"  # Where to save data locally, remove if not necessary
)
print("Chroma DB created successfully.")
retriever = vector_store.as_retriever()
print("Vector DB created successfully.")

# Chat Models (Local only)
# llm_models = {
#     "gemma2-9b-it-local": LlamaCpp(
#         model_path="./models/gemma-2b-it.Q4_K_M.gguf",
#         temperature=0.3,
#         max_tokens=512,
#         n_ctx=2048
#     ),
#     "mistral-local": LlamaCpp(
#         model_path="./models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
#         temperature=0.3,
#         max_tokens=512,
#         n_ctx=2048
#     ),
#     "deepseek-local": LlamaCpp(
#         model_path="./models/deepseek-llm-7b-instruct.Q4_K_M.gguf",
#         temperature=0.3,
#         max_tokens=512,
#         n_ctx=2048
#     ),
#     "claude3-local": LlamaCpp(  # Only if weights are available
#         model_path="./models/claude3-sonnet.Q4_K_M.gguf",
#         temperature=0.3,
#         max_tokens=512,
#         n_ctx=2048
#     )
# }
llm_models = {
    "gemma-7b": Llama.from_pretrained(
        repo_id="google/gemma-7b",
        filename="gemma-7b.gguf",
        n_gpu_layers=60,  # Increase GPU layers to utilize more VRAM
        n_batch=2048,     # Further increase batch size for higher GPU utilization
        n_ctx=4096,   # Keep larger context window
        f16_kv=True,      # Enable half-precision for efficiency
        verbose=False,    # Reduce logging overhead
        n_threads=16,     # Utilize more CPU cores for preprocessing
        gpu_layers_max_thread_use=32,  # Increase threads for GPU layers
        temperature=0.3
    )
}


# Select active model
def get_llm(model_name="gemma-7b"):
    return GLOBAL_LLM

# Memory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Prompts
rag_prompt = PromptTemplate.from_template(
    "Answer the question using the context below:\n\n{context}\n\nQuestion: {question}"
)

fallback_prompt = PromptTemplate.from_template(
    "Answer the question using your own knowledge:\n\nQuestion: {question}"
)
HF_TOKEN="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"
# Logging setup
LOG_FILE = "unanswered_queries.json"
unanswered_log = []
store={}
def get_session_history(session_id:str)->BaseChatMessageHistory:
    if session_id not in store:
        store[session_id]=ChatMessageHistory()
    return store[session_id]

def log_unanswered_query(query):
    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, "w") as f:
            json.dump([], f)
    with open(LOG_FILE, "r+") as f:
        data = json.load(f)
        data.append({"query": query})
        f.seek(0)
        json.dump(data, f, indent=2)

# LangGraph nodes
def retrieve_context(state):
    query = state["question"]
    docs = retriever.get_relevant_documents(query)
    if not docs:
        unanswered_log.append(query)
        log_unanswered_query(query)
    return {
        "question": query,
        "docs": docs,
        "chat_history": state.get("chat_history", []),
        "model_name": state.get("model_name", "gemma-7b")
    }

def route(state):
    return "rag_chain" if state["docs"] else "fallback_chain"

def run_rag(state):
    context = "\n\n".join(doc.page_content for doc in state["docs"])
    chain = LLMChain(llm=GLOBAL_LLM, prompt=rag_prompt)
    answer = chain.run({"context": context, "question": state["question"]})
    return {"answer": answer, "chat_history": state["chat_history"] + [state["question"], answer]}

def run_fallback(state):
    chain = LLMChain(llm=GLOBAL_LLM, prompt=fallback_prompt)
    answer = chain.run({"question": state["question"]})
    return {"answer": answer, "chat_history": state["chat_history"] + [state["question"], answer]}

class State(TypedDict):
  # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    question: str
    docs: Optional[List[Document]]
    chat_history: List[Any]
    model_name: str
    answer: Optional[str]
    messages: Optional[List[Any]]
    
# Define and compile graph
graph = StateGraph(State)
graph.add_node("retrieve", retrieve_context)
graph.add_conditional_edges("retrieve", route)
graph.add_node("rag_chain", run_rag)
graph.add_node("fallback_chain", run_fallback)
graph.add_edge("rag_chain", END)
graph.add_edge("fallback_chain", END)
graph.set_entry_point("retrieve")
compiled_graph = graph.compile()

# Session-aware chatbot interface
# chatbot_graph = RunnableWithMessageHistory(
#     get_session_history,
#     compiled_graph
# )

# Sample run
if __name__ == "__main__":
    session_id = "user_123"
    while True:
        user_question = input("Enter your question (or type 'exit' to quit): ").strip()
        if user_question.lower() == "exit":
            print("Exiting the chatbot. Goodbye!")
            break

        result = compiled_graph.invoke(
            {"question": user_question, "model_name": "gemma-7b"},
            config={"configurable": {"session_id": session_id}}
        )
        print("Answer:", result["answer"])

        if unanswered_log:
            print("\nðŸ›‘ Unanswered Queries Logged:")
            for q in unanswered_log:
                print(" -", q)
